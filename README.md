
# ğŸ™ï¸ Real-Time Emotion-Aware Voice Assistant

A modular, real-time voice assistant that not only understands what users say â€” but how they feel â€” and adapts its responses accordingly using speech emotion recognition and natural language understanding.

> â€œItâ€™s not just what you say, but how you say it â€” your assistant should know the difference.â€

---

## ğŸš€ Features

- ğŸ¤ Real-time voice input
- ğŸ§  Emotion detection from voice tone
- ğŸ’¬ NLP-based intent understanding
- ğŸ­ Emotion-aware adaptive responses
- ğŸ”Š Human-like Text-to-Speech output
- ğŸŒ€ Modular and extensible architecture

---

## ğŸ“š Project Goals

1. Detect user's **emotional tone** from their voice.
2. Understand **intent** using NLP models.
3. Generate a **response** adapted to both emotion and context.
4. Speak the response back using **TTS**.
5. Operate in **real time**, optionally with a terminal or GUI.

---

## ğŸ§  System Architecture

```text
Voice Input â”€â”€â–º Speech-to-Text â”€â”€â–º Emotion Detection â”€â”€â–º Intent Detection â”€â”€â–º Adaptive Response â”€â”€â–º Text-to-Speech
     ğŸ¤                 ğŸ“„                    ğŸ˜¶â€ğŸŒ«ï¸                     ğŸ§                        ğŸ’¬                     ğŸ”Š
```

---

## ğŸ§© Core Modules

| Module           | Function                                             |
|------------------|------------------------------------------------------|
| `capture.py`     | Real-time microphone input                           |
| `stt.py`         | Converts voice to text (STT)                         |
| `emotion.py`     | Detects emotion from voice (and optionally text)     |
| `intent.py`      | Extracts user intent using NLP                       |
| `respond.py`     | Generates responses based on emotion and intent      |
| `tts.py`         | Converts final response to voice                     |
| `app.py`         | Main loop coordinating all modules                   |

---

## ğŸ› ï¸ Tech Stack

- **Language**: Python 3.x
- **Audio**: `sounddevice`, `pyaudio`, `librosa`
- **STT**: Whisper, Vosk, Google STT
- **Emotion Detection**: MFCC + SVM/CNN, or pretrained models
- **NLP**: `transformers`, `spaCy`, `RASA`
- **TTS**: `pyttsx3`, `gTTS`, Tortoise TTS, Bark
- **Async Execution**: `asyncio`, `threading`

---

## ğŸ“ Project Structure

```
emotion_voice_assistant/
â”œâ”€â”€ data/                 # Audio datasets
â”œâ”€â”€ audio/                # Real-time recorded audio
â”œâ”€â”€ models/               # Saved ML models
â”œâ”€â”€ utils/                # Helper scripts
â”œâ”€â”€ core/                 # Main modules
â”‚   â”œâ”€â”€ capture.py
â”‚   â”œâ”€â”€ stt.py
â”‚   â”œâ”€â”€ emotion.py
â”‚   â”œâ”€â”€ intent.py
â”‚   â”œâ”€â”€ respond.py
â”‚   â””â”€â”€ tts.py
â”œâ”€â”€ app.py                # Main runner script
â”œâ”€â”€ index.md              # Project index and roadmap
â””â”€â”€ README.md             # Project overview
```

---

## ğŸ§ª Datasets You Can Use

- RAVDESS
- CREMA-D
- EMO-DB
- TESS

---

## ğŸ“ˆ Future Enhancements

- ğŸ—£ï¸ Expressive voice synthesis (emotional TTS)
- ğŸ‘€ Facial emotion detection (webcam)
- ğŸŒ Multilingual support
- ğŸ§  Context memory for continuous interaction
- ğŸ§ª RLHF (Reinforcement Learning from Human Feedback)

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to fork the repo and submit a PR or create an issue for discussion.

---

## ğŸ“„ License

This project is licensed under the **MIT License**. See `LICENSE` for details.

---

## ğŸ’¡ Credits

Inspired by human conversation, mental health support systems, and advances in speech AI.
